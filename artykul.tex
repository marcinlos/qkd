\documentclass[10pt]{article}


\usepackage{csagh}

\usepackage[utf8]{inputenc}
\usepackage{lmodern} 
\usepackage[T1]{fontenc}
\usepackage{microtype}

\usepackage{url}

\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{braket}
\usepackage[ruled]{algorithm2e}

\usepackage{polski}
\usepackage[polish]{babel}


\newtheorem{theorem}{Theorem}

\begin{document}
\begin{opening}

\title{Quantum Key Distribution}
\author[AGH University of Science and Technology, anna.jagodzinska91@gmail.com]{Anna Jagodzińska}

\begin{abstract}
  (Abstrakt)
\end{abstract}

\keywords{key distribution, quantum cryptography}

\end{opening}

\section{Problem dystrybucji klucza}

Problem dystrybucji klucza w kryptografii to protokół umożliwiający dwóm komunikującym się bytom
uzgodnienie wspólnego tajnego klucza, nie znanego przez nikogo z zewnątrz. Od początku historii
kryptografii dystrybucja klucza stanowiła jeden z najbardziej kłopotliwych praktycznych jej aspektów.
Niezależnie od siły szyfru, bezpieczeństwo klucza jest niezbędne do bezpieczeństwa komunikacji i danych
przez niego chronionych. Przed wynalezieniem kryptografii asymetrycznej do bezpiecznej komunikacji 
potrzebny był obydwu stronom tajny, uzgodniony wcześniej klucz, który musiał w jakiś sposób fizycznie 
zostać przekazany przed rozpoczęciem komunikacji. W czasie drugiej wojny światowej japońska marynarka
przykładowo używała tzw. książek kodowych. Rozwiązanie to miało poważne wady -- ryzyko przechwycenia
ich przez nieprzyjaciela było dość duże, a co za tym idzie, kody musiały być często zmieniane, co 
wymagało ponownego ich drukowania i rozprowadzenia. Szczególnie w trudnych warunkach wojennych,
bezpieczna dystrybucja klucza, od której niejednokrotnie zależało powodzenie operacji militarnych,
a więc życie żołnieży, była zadaniem niełatwym. 

Dzisiaj, jakkolwiek dysponujemy kryptografią asymetryczną, szyfry symetryczne wciąż są szeroko 
stosowane, m. in. ze względu na znacznie większą wydajność\cite{IntelAES, IntelSSL} -- szyfry 
symetryczne zazwyczaj wykorzystują proste operacje bitowe, podczas gdy np. RSA potrzebuje arytmetyki
modularnej\footnotemark. Stąd, kryptografia asymetryczna często wykorzystywana jest do zainicjowania
komunikacji i wygenerowania klucza (np. protokół Diffiego-Hellmana), który następnie jest używany do
szyfrowania symetrycznego. Jednym z najszerzej znanych przykładów takiego schematu jest protokół
SSL, który używa protokołu Diffiego-Hellmana w początkowej fazie komunikacji, by ustalić klucz,
który używany będzie do zapewnienia bezpieczeństwa pozostałej jego części. Po zakończonym sukcesem
procesie uwierzytelniania, do komunikacji używany jest szyfr symetryczny (najczęściej AES).

\footnotetext{
Cytowane materiały Intela pokazują niecałe 4000 operacji na sekundę przy 1024-bitowym kluczu RSA,
oraz czasy rzędu kilku cykli na bajt dla szyfru symetrycznego AES -- różnica to co najmniej 3 
rzędy wielkości.
}


\section{Szyfrowanie one-time pad}
\label{one-time-pad}

Jednym z najbardziej atrakcyjnych potencjalnych przypadków użycia dla ewentualnego w pełni bezpiecznego
protokołu dystrybucji klucza jest z pewnością tzw. szyfr z kluczem jednorazowym (one-time pad). Jego
idea opiera się na wykorzystaniu klucza długości nie mniejszej niż sama szyfrowana wiadomość, i
wykonywaniu prostych operacji na fragmentach szyfrogramu i klucza. Przykład konkretnej implementacji:
tekst jawny i klucz przedstawiane są jako ciągi znaków z pewnego alfabetu alfabetu \(n\)-znakowego 
\(\mathcal{A}\), reprezentowanego przez liczby naturalne \(0,\ldots n-1\), tj. tekst jawny 
\(T=t_1\ldots t_m\), klucz \(K=k_1\ldots k_m\), zaś szyfrogram powstaje poprzez dodawanie odpowiadających
sobie znaków tekstu i klucza modulo \(n\), tj.
\[
S_i=\left(t_i\oplus k_i\right),\qquad a\oplus b \triangleq a+b \mod n
\]
Deszyfracja polega na odwróceniu tej operacji, tj. odejmowaniu kolejnych znaków klucza od znaków
szyfrogramu. Szyfr ten jest całkowicie bezpieczny w sensie teorii informacji -- szyfrogram nie zawiera
żadnej informacji o zaszyfrowanym tekście, poza maksymalną jego długością\cite{Shannon49}. Precyzyjnie
wyrazić to można przy użyciu pojęć teorii informacji -- entropii i entropii warunkowej. Wartości te 
stanowią miarę niepewności, informacji, które niesie ze sobą wystąpienie jakiegoś zdarzenia (w tym
przypadku, wystąpienie danego tekstu jawnego, lub szyfrogramu).
Dla dyskretnej zmiennej losowej \(X\), przyjmującej wartości \(x_1,\ldots,x_n\) z prawdopodobieństwami
\(p(x_1),\ldots,p(x_n)\), entropia zdefiniowana jest jako
\[
\mathcal{H}(X)=-\sum_{i=1}^n p(x_i)\log p(x_i)
\]
zaś dla drugiej zmiennej \(Y\), o wartościach \(y_1,\ldots,y_m\) entropia warunkowa, mierząca niepewność
co do wartości \(X\) gdy dana jest wartość \(Y\), wyraża się poprzez
\[
\mathcal{H}(X\mid Y)=-\sum_{i,j=1}^{n,m}p(x_i,y_j)\log \frac{p(x_i)}{p(x_i,y_j)}
\]
(dla \(p(x_i,y_j)=0\) w sumie przyjmujemy jako składnik \(0\)). Łatwo pokazać, że dla dowolnych \(X\),
\(Y\) jest \(\mathcal{H}(X\mid Y)\leq \mathcal{H}(X)\). Jeśli za \(X\) przyjmiemy zmienną losową
odpowiadającą rozkładowi tekstów jawnych, a za \(Y\) odpowiadających im szyfrogramów, intuicyjnie
\(\mathcal{H}(X\mid Y)\) mierzy jak wiele informacji o tekście jawnym daje znajomość szyfrogramu --
dla małych wartości daje dużo informacji (np. jeśli szyfrogramem byłby sam tekst jawny, z powyższego
wzoru otrzymamy entropię warunkową równą \(0\)), zaś dla dużych -- niewiele. Dowód bezpieczeństwa szyfru
one-time pad polega na pokazaniu, że dla w pełni losowego klucza, entropia warunkowa tekstu jawnego
względem szyfrogramu jest równa entropii samego tekstu jawnego, tj. 
\(\mathcal{H}(X\mid Y) = \mathcal{H}(X)\). Intuicyjnie, szyfrogram nie daje żadnej informacji o tekście
jawnym, ta ,,ukryta'' informacja jest w kluczu.

Powyższe rezultaty odnośnie szyfru z kluczem jednorazowym mają duże historyczne znaczenie teoretyczne.
Aby jednak szyfr był faktycznie bezpieczny, konieczne jest spełnienie szeregu dość restrykcyjnych
założeń.

\begin{itemize}
  \item Klucz musi być długości co najmniej takiej, jakiej jest tekst jawny
  \item Klucz musi być idealnie losowy
  \item Klucz może być wykorzystany tylko raz -- ponowne wykorzystanie klucza umożliwia kryptoanalizę
    opartą np. na niejednorodności rozkładu tekstu jawnego\footnotemark.
\end{itemize}

\footnotetext{
Przykładowo, w ekstremalnym przypadku gdy klucz jest jedną literą, każde wystąpienie danego znaku będzie
odpowiadać takiemu samemu znakowi. Pomijając oczywisty atak typu bruteforce, patrząc na szyfrogram
można z dużym prawdopodobieństwem stwierdzić, które jego znaki odpowiadają najczęściej występującym
w języku, w którym napisany został tekst jawny, porównując po prostu częstotliwość wystąpień.
}

Wymagania te sprawiają, że używanie szyfru z kluczem jednorazowym w praktyce jest dość kłopotliwe.
Szczególnie konieczność stosowania dużych, unikalnych kluczy, które obydwie strony komunikacji muszą
posiadać przed jej rozpoczęciem. Wymaga to wcześniejszego przygotowania -- gdyby istniał sposób
bezpiecznej transmisji klucza rozmiaru samej wiadomości bez możliwości przechwycenia go przez 
potencjalnych napastników, równie dobrze można by użyć takiego kanału do przekazania samej wiadomości.

Mimo tych problemów, szyfry z kluczem jednorazowym były stosunkowo popularne i często wykorzystywane
w sytuacjach, gdzie komunikacja nie była częsta, a niezbędny był bardzo wysoki stopień bezpieczeństwa,
np. przez sowieckich szpiegów w okresie zimnej wojny. Problemem była oczywiście dystrybucja kluczy
-- przechowywane były one na bardzo małych, łatwych do ukrycia kartkach papieru, często nasączonych
substancjami łatwopalnymi, by ułatwić natychmiastowe ich zniszczenie bez zostawienia śladów po użyciu.

Mimo to, trudno wyobrazić sobie używanie takiego systemu współcześnie, do szyfrowania komunikacji
,,na codzień''. Fizyczna dystrybucja kluczy zdaje się być zbyt niepraktycznym rozwiązaniem w sytuacji,
gdy komunikacja następuje pomiędzy wieloma parami uczestników, dynamicznie, i przesyłane są duże
ilości danych. Co więcej, zostało udowodnione\cite{Shannon49}, że każdy szyfr bezpieczy z punktu
widzenia teorii informacji ma podobnie kłopotliwe wymagania. W obliczu braku w pełni bezpiecznych,
szybkich i wygodnych protokołów dystrybucji klucza zmuszeni jesteśmy korzystać z szyfrów, których
bezpieczeństwo oparte jest jedynie o przypuszczalną\footnotemark trudność obliczeniową pewnych 
problemów algorytmicznych, jak faktoryzacja, czy obliczanie logarytmu dyskretnego.

\footnotetext{
Pomijając kwestię P vs NP, w przypadku której, jakkolwiek pozostaje niepewność, większość ekspertów
jest mocno przekonana, że równość nie zachodzi, w przypadku problemów najczęściej leżących u podstaw
algorytmów kryptograficznych -- faktoryzacji i obliczania logarytmu dyskretnego -- nie wiadomo nawet,
czy są NP-zupełne.
}

\section{Klasyczne metody generowania klucza}

W tej sekcji przyjrzymy się klasycznemu algorytmowi dystrybucji (tworzenia) klucza, opartemu
na trudności pewnych problemów obliczeniowych.

\subsection{Protokół Diffiego-Hellmana}

Przypuszczalnie najszerzej wykorzystywanym dziś protokołem uzgadniania klucza jest protokół 
Diffiego-Hellmana\cite{dh76}. Pozwala on dwóm komunikującym się stroną bezpiecznie stworzyć wspólny, 
tajny klucz tak, że ewentualny podsłuchiwacz, zdolny przechwycić całość komunikacji w obydwie strony,
nie jest w stanie w prosty sposób wyznaczyć jego wartości.

Załóżmy, że Alicja i Bob chcą stworzyć wspólny tajny klucz. Niech \(G=\langle g\rangle\) będzie grupą
cykliczną generowaną przez \(g\), znaną powszechnie -- Bob, Alicja, i ewnetualny podsłuchiwacz -- Ewa
-- znają \(G\) i \(g\). Protokół ma następujący przebieg:

\begin{enumerate}
  \item Alicja wybiera liczbę \(a\in\mathbb{N}\), oblicza \(g^a\) i wysyła Bobowi
  \item Bob wybiera liczbę \(b\in\mathbb{N}\), oblicza \(g^b\) i wysyła Alicji
  \item Alicja, znając \(a\) i \(g^b\), oblicza \(g^{ab}=\left(g^a\right)^b\)
  \item Bob, analogicznie, oblicza \(g^{ab}=\left(g^b\right)^a\)
\end{enumerate}

Tajny klucz to wartość \(g^{ab}\). Ewa jest w posiadaniu wartości \(g^a\) oraz \(g^b\), ale nie są
znane efektywne sposoby obliczenia \(g^{ab}\) na podstawie tych informacji. Problem jego wyznaczenia
znany jest w literaturze jako problem Diffiego-Hellmana\footnotemark. W praktyce jako grupę \(G\)
zazwyczaj wybiera się podgrupę multiplikatywną pewnego ciała Galois \(\mathbb{Z}/p\mathbb{Z}\), a
obliczenia sprowadzają się do znanej arytmetyki modularnej.

\subsection{Wersja dla więcej niż dwóch uczestników}

Zaprezentowany powyżej sposób postępowania można łatwo rozszerzyć na większą ilość uczestników,
obliczając w ogólności \(g^{a_1 \cdots a_n}\), gdzie \(a_1,\ldots,a_n\) to losowo wybrane przez nich
liczby. Protokół przebiega bardzo podobnie -- obliczane są i przekazywane odpowiednim osobom
wartości potęg \(g\) odpowiadających każdym podzbiorom właściwym zbioru 
\(\left\{a_1,\ldots,a_n\right\}\), t.j. \(g^{a_S}\), gdzie
\[
a_S = \prod_{i\in S}a_i,\ \ \ 
\emptyset\neq S\varsubsetneq \left\{1,\ldots,n\right\}
\]
(dla przypadku \(n=2\) były to \(\left\{a_1=a\right\}\) oraz
\(\left\{a_2=b\right\}\)). Podsłuchujący zna wszystkie te wartości, ale podobnie jak w wariancie
podstawowym, nie jest w stanie odtworzyć z nich prosto samego \(g^{a_1 \cdots a_n}\).

\section{Problemy}

Całe bezpieczeństwo protokołu opiera się na trudności problemu Diffiego-Hellmana. Rozwiązanie 
problemu obliczania logarytmu dyskretnego (DLP) wystarcza do obliczenia klucza, pozwala 
bowiem obliczyć \(a\) na podstawie znajomości \(g^a\), zatem problem Diffiego-Hellmana jest nie 
trudniejszy niż DLP, jednak redukcja w drugą stronę w ogólnym przypadku nie jest znana\footnotemark.
W przypadku problemu logarytmu dyskretnego niewiele wiadomo o faktycznej jego złożoności -- w
szczególności, nie wiadomo, czy problem jest NP-zupełny -- jednak jak dotąd nie jest znany algorytm
wielomianowy (przegląd istniejących algorytmów znaleźć można np. w\cite{Sutherland07}), więc uznać
można go w chwili obecnej za problem praktycznie trudny.

\footnotetext{
Ogólny przypadek zdaje się pozostawać problemem otwartym, natomiast istnieją wyniki częściowe.
Dla pewnej klasy liczb pierwszych zostało pokazane, że problem Diffiego-Hellmana jest nie prostszy
niż problem logarytmu dyskretnego\cite{Goldwasser90}. 
}

Warto również zaznaczyć, że jakkolwiek klasyczny wielomianowy algorytm dla problemu logarytmu 
dyskretnego nie jest znany, to istnieje efektywny algorytm kwantowy\cite{Shor97}, zatem ewentualny 
rozwój komputerów kwantowych wymusi tak czy inaczej zarzucenie użycia protokołu Diffiego-Hellmana
\footnotemark.

\footnotetext{
Na podobną przypadłość cierpi dużo znanych i powszechnie używanych obecnie algorytmów kryptografii
asymetrycznej, m. in. RSA -- efektywny kwantowy algorytm faktoryzacji opisany jest w tej samej
pracy\cite{Shor97}.
}

\section{Quantum Key Distribution}

\subsection{Początki kryptografii kwantowej}

Od samego początku swojego rozwoju jako dyscypliny czysto matematycznej, kryptografia nierozłącznie
związana była z teorią informacji i informatyką teoretyczną, w szczególności teorią obliczeń. Przy
użyciu jej narzędzi można było nie tylko analizować istniejące rozwiązania, ale także ściśle
dowodzić pewnych fundamentalnych ograniczeń przyjętego modelu. Brak możliwości uzyskania silniejszych
gwarancji bezpieczeństwa (często poparty matematycznymi dowodami, jak w przypadku wymagań dla 
szyfrowania doskonale bezpiecznego) skłaniał do tworzenia rozwiązań opartych o ,,trudne'' problemy
obliczeniowe. 

W tym okresie możliwe było już precyzyjne wyrażanie
takich idei. Pojęcie algorytmu, intuicyjnie rozumiane już od starożytności, w miarę rozwoju 
matematyki coraz głośniej domagało się ścisłej definicji. Dziesiąty problem Hilberta\cite{Gray01}
dotyczy algorytmu rozwiązywania wielomianowych równań Diofantycznych. O ile oczekiwana odpowiedź 
mogła zostać łatwo udzielona poprzez jego znalezienie i udowodnienie poprawności (nikt prawdopodobnie
nie miałby wątpliwości, czy zaprezentowany rezultat jest faktycznie ,,algorytmem''), to pokazanie,
że taki algorytm nie istnieje\footnote{Co też faktycznie ma miejsce -- dziesiąty problem Hilberta
rozwiązany został w 1970 r. poprzez udowodnienie, że żądany algorytm nie może istnieć 
\cite{Matiyasevich70}} bez ścisłej definicji algorytmu byłoby niemożliwe -- szczególnie,
że nikt w tamtym okresie nie spodziewał się takiego rozwiązania (Hilbert nie pyta nawet o istnienie
-- pyta o algorytm\footnotemark). Formalne definicje obliczenia, takie jak maszyna Turinga, czy rachunek
lambda A. Churcha powstały niedługo potem. Są w swej naturze ,,skończone'' i ,,dyskretne'', mocno
algebraiczne. Do dziś zdają się stanowić adekwatny opis naszej intuicji odnośnie algorytmu. Rozwój 
fizyki i mechaniki kwantowej pokazały jednak, że nasza intuicja o świecie, jakkolwiek przydatna 
i skuteczna w makroskali, nie stanowi bynajmniej poprawnego jej opisu. Zaprzęgnięcie często 
nieintuicyjnych, niemal paradoksalnych, potwierdzonych mimo to eksperymentalnie wniosków z teorii 
mechaniki kwantowej do tworzenia narzędzi do obliczeń i komunikacji wymaga stworzenia i pracy w 
zupełnie nowym modelu kryptografii, dającym nowe możliwości, i pozbawionym pewnych ograniczeń modelu
klasycznego.
 
\footnotetext{
Hilbert głęboko wierzył, że sformalizowanie matematyki umożliwi prędzej czy później udzielenie
definitywnej odpowiedzi na wszelkie pytania. Sporą część kariery poświęcił na rozwój logiki i 
systemów formalnych. Jak na ironię, rezultaty burzące nadzieje na osiągnięcie w pełni tego, o czym
marzył, pojawiać zaczęły się kilka lat przed jego śmiercią (m. in. twierdzenia G\"{o}dla o 
niezupełności), szydząc niejako z jego słynnego epitafium -- \emph{,,Wir müssen wissen.
Wir werden wissen''}
}

\subsection{Przewaga QKD nad rozwiązaniami klasycznymi}

Przy rozważaniu protokołów dystrybucji klucza zakładamy zazwyczaj, że przeciwnik -- Ewa -- ma możliwość
podsłuchania całej komunikacji pomiędzy jego uczestnikami. Jest to założenie uzasadnione -- gdyby nie
możliwość podsłuchania komunikacji, żadne zabezpieczenia nie byłyby wymagane. Trudno też z całą 
pewnością postawić ograniczenie górne na ilość podsłuchanych informacji, ani wykryć (przynajmniej
natychmiast) fakt podsłuchiwania, więc pełne bezpieczeństwo dać może jedynie protokół, który jest 
bezpieczny nawet w tak pesymistycznym przypadku. 

Podsłuchiwanie transmisji przez kanał kwantowy jednakowoż napotyka na istotną przeszkodę -- obserwacja
wymaga pomiaru, który zmienia przesyłany stan. W połączeniu z twierdzeniem o zakazie klonowania 
uniemożliwia to podsłuchiwaczowi działanie pozbawione wpływu na przesyłane dane. Okazuje się, że własność
ta pozwala konstruować protokoły, w których uczestnicy natychmiast dowiadują się nie tylko o fakcie
podsłuchiwania, ale także o tym, ile i jakie konkretnie informacje zostały przechwycone. Konkretne 
protokoły różnią się sposobem wykrywania podsłuchu i reakcją nań, ale możliwość ta stanowi główny 
czynnik odróżniający kwantową dystrybucję klucza od klasycznej, cechę unikalną dla niej i źródło jej 
siły.

\section{Klasyfikacja protokołów QKD}

\subsection{Mechanizmy wykrywania podsłuchu}

Istnieje wiele opublikowanych protokołów wymiany klucza, różniących się szczegółami działania, ale
większość zaklasyfikować można do jednej z dwóch grup, w zależności od mechanizmu mechaniki
kwantowej, który wykorzystują do zapewnienia bezpieczeństwa.

\textbf{Prepare-and-measure} wykorzystuje fakt, że ewentualny podsłuchiwacz nie jest w stanie zapewnić,
że do prawowitego odbiorcy komunikacji trafią qubity takie same, jak te, które zostały wysłane.
Gwarantuje to połączenie dwóch faktów: po pierwsze, podsłuchiwacz nie jest w stanie uzyskać informacji
(t.j. dokonać pomiaru) bez zburzenia jej (efekt obserwatora), po drugie zaś nie jest w stanie wykonać
kopii przechwyconego stanu, na której mógłby wykonać pomiar zamiast na nim (tw. o zakazie klonowania).
Historycznie rodzina ta pojawiła się jako pierwsza, należy do niej pierwszy opublikowany protokół
kwantowej dystrybucji klucza -- BB84\cite{bb84,Scarani09}.

\textbf{Protokoły oparte na splątaniu} wykorzystuje w komunikacji stany splątane i fakt, że 
podsłuchiwacz, wykonując pomiar, wpływa wówczas na stan całego systemu. Pozwala to także na określenie
ilości informacji, które zostały przechwycone. Przykładowo omówiony w dalszej części, historycznie
drugi protokół A. Ekerta wykrywa podsłuch poprzez wykorzystanie do komunikacji dwuqubitowych systemów
w stanie Bella testując nierówności Bella \(\frac{1}{\sqrt{2}}\left(\Ket{01}+\Ket{10}\right)\) 
-- jeśli korelacja wyników nie narusza nierówności Bella, stan systemu uległ zmianie w wyniku działań
zewnętrznych\cite{Ekert91,Ekert12}.

\subsection{Mechanizm odbioru informacji}

Kolejny element różniący zaproponowane praktyczne protokoły QKD to sposób odbioru informacji.
Rozróżniamy trzy podstawowe rodziny: zmiennej dyskretnej, zmiennej ciągłej, oraz \emph{distributed
phase reference}\cite{Scarani09}.

\subsubsection*{Kodowanie zmiennej dyskretnej (\emph{discrete variable})}

Jest to najstarsza rozważana rodzina protokołów. Obydwa pierwotne protokoły -- BB84 i E91 -- do niej 
należą. Również dzisiaj cieszy się dużą popularnością, i najczęściej wykorzystywana jest w faktycznych
implementacjach. W protokołach typu zmiennej dyskretnej informacje przenoszone są przez pojedyncze 
cząstki. Zazwyczaj do komunikacji wykorzystywane są spolaryzowane fotony. Narzędzie odbioru informacji
stanowi licznik fotonów.

Na przestrzeni lat powstało kilka innych protokołów wykorzystujących kodowanie zmiennej dyskretnej.
Warto wśród nich wymienić B92 -- uproszczoną wersję BB84\cite{Bennett92}. Główna różnica polega na
wykorzystaniu dwóch stanów qubitu, zamiast czterech. Kolejny ciekawy pomysł to SARG04\cite{Scarani04}.
Podobnie jak BB84, udowodnione zostało jego bezwarunkowe bezpieczeństwo. Oparty jest na pomyśle 
podobnym do BB84, jednak różnice w procesie uzgadniania klucza po samej komunikacji sprawiają, że jest
znacznie bardziej odporny na pewną klasę ataków (PNS -- więcej w sekcji dot. bezpieczeństwa), na którą 
ze względów praktycznych podatny jest BB84, dzięki czemu jest bezpieczniejszy i prostszy w implementacji.


\subsubsection*{Kodowanie zmiennej ciągłej (\emph{continuous variable})}

Protokoły zmiennej dyskretnej nie są pozbawione wad. Jedną z nich jest efektywność -- jakkolwiek
wymagana do ich implementacji technologia od dawna istnieje, to liczniki fotonów są stosunkowo
powolne i stratne w stosunku do zaproponowanych później rozwiązań\cite{Reid00,Cerf01}. Protokoły
zmiennej ciągłej oparte są na efektach i pomiarach optycznych przy użyciu tzw. odbiornika homodynowego,
umożliwiającego pomiar składowej kwadraturowej odbieranego sygnału. Pomiary takie przeprowadzane
są znacznie bardziej wydajnie i niezawodnie, aniżeli wykrywanie fotonów.

Mimo odmiennej natury pomiarów, bezpieczeństwo komunikacji gwarantują te same zasady, co w przypadku
zmiennej dyskretnej. Przykładowo, protokół zaproponowany w\cite{Reid00} opiera się na korelacjach 
podobnych do tych opisanych przez Einsteina, Podolskiego i Rosena dla cząstek, jednak obowiązujących 
dla składników kwadraturowych całych wiązek, mimo przestrzennej separacji (podobnie jak w protokole 
Ekerta).

Ważną podrodzinę rodziny protokołów zmiennej ciągłej stanowią protokoły Gaussowskie, wykorzystujące
stany światła z modulacją o rozkładzie Gaussowskim. Jednym z pierwszycht takich protokołów jest 
\cite{Cerf01}. Więcej przykładów wraz z krótkim opisem znaleźć można w\cite{Scarani09}.


\subsubsection*{Distributed phase reference}

W protokołach typu \emph{distributed phase reference} przekazywane informacje są dyskretne, jak
w przypadku ze zmienną dyskretną, nieco inne jest jednak ich kodowanie. Wartości kolejnych bitów
przesyłanej informacji nie odpowiadają bezpośrednio wartościom nadawanych sygnałów, ale są w jakiś
sposób zawarte w ciągu. Rozważania tego typu, w przeciwieństwie do dwóch pozostałych rodzin,
swe źródło mają w praktycznych kwestiach implementacyjnych, w szczególności trudności związane
z zaburzeniami fazy w BB84 przy przesyłaniu spolaryzowanych fotonów na duże odległości przy
użyciu światłowodu\cite{Liu13}.

Pierwszy algorytm tego typu to \emph{Differential Phase-Shift} (DPS)\cite{Inoue02}. Wartości
kolejnych bitów zakodowane są w różnicy fazy kolejno przesyłanych sygnałów. Zostało udowodnione,
że jest bezwarunkowo bezpieczny\cite{Wen09}. Istnieją też działające implementacje\cite{Liu13}.

Kolejnym obiecującym protokołem z tej dziedziny jest \emph{Coherent One-Way} (COW)\cite{Stucki05}.
Działa w oparciu o pomiary czasu przybycia sygnału. Jest stosunkowo wydajny i prosty w implementacji.
Podobnie jak DPS, został przetestowany w praktyce, z całkiem dobrym rezultatem\cite{Stucki08}.


\section{Mechanizmy pomocnicze}

\subsection{Korekta błędów kwantowych}

Błędy w komunikacji przy użyciu kanału kwantowego pojawić się mogą nie tylko na skutek ingerencji
osób trzecich. Nie sposób w pełni wyeliminować błędów wynikających z niedoskonałości urządzeń
używanych w komunikacji. Podobnie jak w komunikacji tradycyjnej, konieczne jest użycie metod
umożliwiających ich wykrywanie i korektę. Wyzwanie stanowi fakt, że konieczna jest w tym celu
komunikacja na kanale klasycznym, podsłuchiwanym z założenia w pełni i bezkarnie przez Ewę -- 
pożądana zatem jest minimalizacja informacji w ten sposób ujawnionych.

Jednym z najpopularniejszych rozwiązań tego problemu jest tzw. protokół \emph{Cascade}\cite{Brassard94}.
W przypadku kanałów o nawet stosunkowo dużym współczynniku błędu (ilość błędnie przekazanych wartości
\(\leq 15\%\)), pozwala on w czasie wielomianowym usunąć niezgodności danych, ujawniając przy tym
ilość informacji bliską minimalnej możliwej. Algorytm polega mniej więcej na wielokrotnym dzieleniu 
danych na bloki, obliczaniu bitów parzystości i ewentualnej korekcie poprzez lokalizację błędu przy
użyciu wyszukiwania binarnego (sprawdzając parzystości podbloków). Szczegółowy opis zostanie tu
pominięty, znaleźć można go w cytowanym artykule, wraz z dokładną analizą\cite{Brassard94}.


\subsection{Privacy amplification}

Załóżmy, że Alicja i Bob wykonali już część główną protokołu, i dysponują zestawem danych, oraz
informacją, jaka ich część została przechwycona przez podsłuchiwacza -- Ewę. Nie są jednak w stanie
stwierdzić, która część danych została przechwycona. Aby więc protokoły tego rodzaju miały jakieś
praktyczne zastosowanie, konieczny jest sposób przekształcenai tych częściowo przechwyconych informacji
w klucz mniejszy, ale taki, o którym Ewa nie posiada żadnych informacji. Metody pozwalające to osiągnąć
znane są jako \emph{privacy amplification}\cite{Watanabe07}.

Dla ilustracji rozważmy prosty przykład: Alicja i Bob posiadają klucz dwubitowy \(r = b_1 b_2\), oraz
wiedzą, że Ewa przechwyciła jeden z bitów. Alicja i Bob decydują jako klucza właściwego użyć alternatywy
wykluczającej posiadanych bitów, \(r' = b_1 \oplus b_2\). Łatwo zauważyć, że znajomość jednego bitu
nie daje Ewie żadnych informacji, bo przy ustalonym jednym bicie wartość \(r'\) zależy w całości od 
drugiego, nieznanego. Alicja i Bob mogą więc być pewni, że klucz \(r'\) może być użyty do bezpiecznej
komunikacji. W faktycznych protokołach Alicja i Bob nie mają nigdy pewności, co do ilości przechwyconych 
danych, konieczne jest zatem wzięcie pod uwagę również rozkład prawdopodobieństwa tej informacji. 
Formalnie tego typu stwierdzenia wyrażać i dowodzić można w ramach teorii informacji, przy użyciu pojęć
entropii i entropii względnej, wprowadzonych w sekcji \ref{one-time-pad}.

Istnieją dwie główne rodziny metod privacy amplification -- oparte o kodowanie liniowe oraz o
haszowanie uniwersalne.

\subsubsection*{Kody liniowe}
Najpopularniejsza metoda wykorzystuje kody liniowe do kompresji częściowo przechwyconego klucza\cite{Mayers01}.
Klucze reprezentowane są jako ciągi bitów, t.j. wektory w \(\mathbb{F}_2^n\). Do klucza początkowego,
częściowo przechwyconego \(u\in\mathbb{F}_2^n\) aplikowane jest odpowiednio dobrane przekształcenie 
liniowe \(\tau\colon\mathbb{F}_2^n\rightarrow \mathbb{F}_2^m\) (czyli macierz binarna \(n\times m\)), 
wybrane m. in. na podstawie ilości podsłuchanych danych. Stosując odpowiednie macierze, można udowodnić
wystarczająco silne ograniczenia dolne na entropię warunkową \(\tau(u)\) względem informacji,
którymi dysponuje Ewa.

\subsubsection*{Uniwersalne funkcje haszujące}

Druga rodzina wykorzystuje do tworzenia klucza uniwersalne funkcje haszujące\cite{Bennett95,Watanabe07}.
Wybierając losową funkcję haszującą z pewnej klasy uniwersalnej i obliczając jej wartość na kluczu
otrzymujemy podobnie jak w przypadku kodów liniowych krótszy klucz o dużej entropii. Jest to w pewnym
sensie metoda ogólniejsza niż użycie kodów liniowych, bowiem zbiór wszystkich przekształceń liniowych
\(\mathbb{F}_2^n\rightarrow \mathbb{F}_2^m\) stanowi uniwersalną klasę funkcji haszujących 
\cite{Carter79}.

\vspace{3mm}

Oczywiście, techniki privacy amplification nie zwiększają w żaden sposób entropii warunkowej danych
Alicji i Boba względem tego, co zostało przechwycone -- pozwalają jedynie ,,skompresować'' ją w
mniejszym kluczu. Może zdażyć się, że ilość informacji podsłuchanych przez Ewę nie pozostawia 
wystarczającej ilości entropii do wygenerowania odpowiednio długiego klucza. Wówczas konieczne jest
przerwanie protokołu. 


\section{Protokoły QKD}

W tej sekcji omówione zostaną przykładowe protokoły QKD. Ogólne założenia odnośnie sytuacji są
następujące: Alicja i Bob chcą stworzyć wspólny klucz, nieznany podsłuchiwaczowi -- Ewie. Dysponują
dwoma kanałami informacji: klasycznym, który jest w pełni bez przeszkód podsłuchiwany przez Ewę,
oraz kwantowym, podlegającym prawom i ograniczeniom mechaniki kwantowej. Kanał klasyczny jest
uwierzytelniony -- Alicja i Bob są w stanie zidentyfikować się nawzajem, Ewa widzi całą komunikację,
ale nie jest w stanie w niej uczestniczyć\footnotemark. Kanał kwantowy jest otwarty na wszelkie 
manipulacje ze strony Ewy. Celem protokołu jest uzgodnienie tajnego klucza, bądź zerwanie komunikacji,
jeśli okaże się to niemożliwe.

\footnotetext{
Innymi słowy, na kanale klasycznym nie jest możliwy atak typu man-in-the-middle.
}

\subsection{BB84}

Pierwszy\cite{Scarani09} protokół kwantowej dystrybucji klucza został opublikowany w 1984 przez
C. Benneta i G. Brassarda\cite{bb84}. 

Rozważmy dwie bazy przestrzeni qubitów -- \(\mathcal{B}_1 = \left\{\ \Ket{+x}, \Ket{-x}\,\right\}\), 
będące ortonormalnymi wektorami własnymi operatora pomiaru \(\sigma_x\) oraz 
\(\mathcal{B}_2 = \left\{\ \Ket{+z}, \Ket{-z}\,\right\}\), będące ortonormalnymi wektorami własnymi
operatora pomiaru \(\sigma_z\), gdzie \(\sigma_z\), \(\sigma_z\) to operatory Pauliego. W praktyce
odpowiadają one stanom polaryzacji fotonów, wykorzystywanych w implementacji. Protokół przebiega 
następująco:

\begin{enumerate}
  \item Alicja wybiera losowo \(n\) bitów \(a=a_1\ldots a_n\)
  \item Dla każdego \(i\in\left\{1,\ldots,n\right\}\):
    \begin{enumerate}
      \item Alicja wybiera losowo jedną z baz -- \(\mathcal{B}_{b_i}\)
      \item Alicja przygotowuje qubit w stanie odpowiadającym \(a_i\)-temu elementowi wybranej
        bazy i wysyła go Bobowi
      \item Bob wybiera losowo jedną z baz -- \(\mathcal{B}_{\tilde{b}_i}\) i mierzy w niej
        odebrany qubit, otrzymując wartość \(\tilde{a}_i\).
    \end{enumerate}
  \item Na kanale klasycznym Alicja i Bob wymieniają informację o wybranych w poprzednim
    kroku bazach, t.j. \(b_1,\ldots,b_n\) oraz \(\tilde{b}_1,\ldots,\tilde{b}_n\), odrzucają te
    wartości, dla których ich bazy się nie zgadzają
  \item Na kanale klasycznym Alicja i Bob ujawniają pewną część swoich kluczy -- wartości 
    \(a_i\), \(\tilde{a}_i\) -- aby oszacować, jaka część komunikacji została podsłuchana
  \item Jeśli wystarczająco mała, używają jakiegoś schematu privacy amplification do wygenerowania
    bezpiecznego klucza, w przeciwnym wypadku protokół kończy się niepowodzeniem
\end{enumerate}

Po kroku 3, Alicja i Bob dysponują w przybliżeniu \(N/2\) qubitami nadanymi i odebranymi w tej samej
bazie. W przypadku braku ingerencji z zewnątrz, ich wartości \(a_i\) i \(\tilde{a}_i\) powinny być
identyczne. Jeśli jednak Ewa zdecyduje się na prosty atak \emph{intercept-and-resend}, polegający
na zmierzeniu qubitu nadanego przez Alicję i odesłaniu rezultatu do Boba, z dużym prawdopodobieństwem
pojawią się różnice. Konkretnie, jeśli Ewa zmierzy qubit w odpowiedniej bazie, takiej, w jakiej
przygotowała go Alicja, to wynik pozostanie niezmieniony, i qubit, który trafi do Boba, będzie taki
sam jak ten, który wysłała Alicja. Jeśli jednak Ewa zmierzy qubit w innej bazie, stanie się inaczej.
Między wartościami własnymi -- stanami qubitu po pomiarze -- zachodzi relacja
\[
\Ket{\pm x}=\frac{1}{\sqrt{2}}\left(\Ket{+z}\pm\Ket{-z}\right)
\]
i podobnie dla \(\Ket{\pm z}\). Stąd, w wyniku pomiaru w niewłaściwej bazie otrzymamy stan, który
po wykonaniu następnie pomiaru w bazie właściwej da jeden z dwóch możliwych wyników z takim samym
prawdopodobieństwem. Jeśli więc Ewa wykona pomiar w niewłaściwej bazie, i prześle wynik do Boba,
który wykona pomiar w bazie właściwej, w połowie przypadków uzyska niepoprawny wynik. Podsumowując,
Ewa wykonując pomiar wprowadza błąd w \(1/4\) przypadków. Prawdopodobieństwo, że jej manipulacje
pozostaną niewykryte spada więc wykładniczo z ilością przesyłanych qubitów. Poznaje natomiast 
poprawną wartość połowy przechwyconych qutibów. 

Okazuje się, że to wystarczająco dużo, by była w stanie uniemożliwić zakończenie protokołu sukcesem.
Jeśli Ewa przechwytuje w ten sposób co najmniej \(\approx 70\%\) komunikacji na kanale kwantowym, to 
nie da się skonstruować wspólnego tajnego klucza -- Ewa ma więcej informacji o przesyłanych wartościach, 
niż Bob\cite{Csiszar78,Scarani09}. Wciąż jednak nie jest w stanie uzyskać ich bez wiedzy Alicji
i Boba, więc bezpieczeństwo jest zachowane.

\subsection{E91}

Kilka lat później, niezależnie, kolejny protokół stworzony został przez A. Ekerta\cite{Ekert91,Ekert12}.
Podobnie jak BB84, wykorzystywał do komunikacji spolaryzowane fotony, mierzone w różnych bazach,
jednak zasada jego działania, a w szczególności mechanizm wykrywania podsłuchu jest inny.

W protokole używane są po trzy różne bazy dla Alicji i Boba, odpowiadające rotacji pewnej wybranej
bazy \(\mathcal{B}\) o \(0\), \(\frac{1}{4}\pi\), \(\frac{1}{2}\pi\) dla Alicji, i \(\frac{1}{4}\pi\), 
\(\frac{1}{2}\pi\) i \(\frac{3}{4}\pi\) dla Boba. Kluczowym elementem implementacji protokołu jest 
źródło splątanych par qubitów w jednym ze stanów Bella:
\[
\frac{1}{\sqrt{2}}\left(\Ket{01}+\Ket{10}\right)
\]
w bazie \(\mathcal{B}\). Intuicyjnie, para qubitów ma przeciwny spin, natomiast przed pomiarem nie 
sposób stwierdzić, która ma który. Dzięki temu, jeśli Alicja wykona pomiar i wymusi w ten sposób 
,,zapadnięcie się'' systemu do stanu czystego to Bob, jeśli użyje do pomiaru kompatybilnej bazy, otrzyma
zawsze wynik przeciwny. Tak przygotowane qubity wysyłane są do Alicji i Boba. Natura źródła atura nie 
jest  istotna (może nim być np. jeden z uczestników komunikacji), ważne są tylko własności par, które
emituje. Protokół przebiega następująco\cite{Ilic07}:

\begin{enumerate}
  \item Źródło tworzy \(n\) par splątanych qubitów w stanie \(\frac{1}{\sqrt{2}}\left(\Ket{01}+
    \Ket{10}\right)\), wysyła je do Alicji i Boba
  \item Dla każdego \(i\in\left\{1,\ldots,n\right\}\):
    \begin{enumerate}
      \item Alicja wybiera jedną ze swoich baz, i mierzy w niej \(i\)-ty qubit
      \item Bob wybiera jedną ze swoich baz, i mierzy w niej \(i\)-ty qubit
    \end{enumerate}
  \item Przez kanał klasyczny Alicja i Bob ujawniają swoje wybrane bazy
  \item Alicja i Bob wymieniają poprzez kanał klasyczny informacje o wynikach pomiarów dla tych
    par qubitów, dla których ich bazy się nie zgadzały
  \item Na podstawie tych informacji określają korelację swoich wyników, aby stwierdzić, czy pary
    splątane przesłane przez źródło nie uległy po drodze manipulacji
  \item Na podstawie oszacowanej ilości przechwyconych przez Ewę informacji, Alicja i Bob tworzą
    klucz, lub protokół kończy się niepowodzeniem 
\end{enumerate}

Podobnie jak w przypadku BB84, Alicja i Bob mogą ujawnić pewne fragmenty swoich danych, by oszacować
stopień błędu, który mógł powstać nawet bez udziału podsłuchiwacza, na skutek niedoskonałości systemu.
Kluczowy jest krok 4, który pozwala wykryć interencję Ewy i oszacować jej rozmiar. Odbywa się to
poprzez porównanie obliczonych korelacji pomiędzy wynikami z wartościami przewidywanymi przez mechanikę
kwantową. Dla splątanych qubitów w stanie podanym powyżej korelacje powinny być wysokie (w szczególności,
nierówności Bella powinny być pogwałcone), natomiast ingerencja Ewy spowodować musi ich zmniejszenie.

\nocite{*}

\bibliographystyle{cs-agh}
\bibliography{bibliography}

\end{document}
